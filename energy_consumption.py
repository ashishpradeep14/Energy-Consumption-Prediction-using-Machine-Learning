# -*- coding: utf-8 -*-
"""energy consumption.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gPzR2ThEEnJUoYbeTYztxipur1zP_lIn

# **Importing libraries**
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load data
data = pd.read_csv("/content/energydata_complete.csv")

data.head(50)

data.info()

data.describe()

data.isnull().sum()

# Convert date column to datetime
# Specifying the correct format
data['date'] = pd.to_datetime(data['date'], format='%d-%m-%Y %H:%M') #Changed the format

# Extract time-based features
data['hour'] = data['date'].dt.hour
data['day'] = data['date'].dt.day
data['month'] = data['date'].dt.month
data['weekday'] = data['date'].dt.weekday



"""# **Feature Engineering**"""

# Create weekend flag
data['is_weekend'] = data['weekday'].apply(lambda x: 1 if x >= 5 else 0)

# Create part of day feature
def part_of_day(hour):
    if 5 <= hour < 12:
        return "morning"
    elif 12 <= hour < 17:
        return "afternoon"
    elif 17 <= hour < 21:
        return "evening"
    else:
        return "night"

data['part_of_day'] = data['hour'].apply(part_of_day)
data = pd.get_dummies(data, columns=['part_of_day'], drop_first=True)

# Create season feature
def get_season(month):
    if month in [12, 1, 2]:
        return "winter"
    elif month in [3, 4, 5]:
        return "spring"
    elif month in [6, 7, 8]:
        return "summer"
    else:
        return "autumn"

data['season'] = data['month'].apply(get_season)
data = pd.get_dummies(data, columns=['season'], drop_first=True)

# Interaction between indoor and outdoor temperatures
data['T1_Tout_diff'] = data['T1'] - data['T_out']

# Interaction between humidity and temperature
data['T1_RH1_interaction'] = data['T1'] * data['RH_1']
data['To_RHout_interaction'] = data['T_out'] * data['RH_out']

# Rolling average (window size = 3)
data['T1_rolling_mean'] = data['T1'].rolling(window=3, min_periods=1).mean()
data['RH_1_rolling_mean'] = data['RH_1'].rolling(window=3, min_periods=1).mean()

# Lag features for target variable
data['Appliances_lag_1'] = data['Appliances'].shift(1)
data['Appliances_lag_2'] = data['Appliances'].shift(2)

# Fill NaN values from lagging
data.fillna(method='bfill', inplace=True)

# Temperature and humidity ratios
data['T_in_out_ratio'] = data['T1'] / (data['T_out'] + 1e-6)
data['RH_in_out_ratio'] = data['RH_1'] / (data['RH_out'] + 1e-6)

# Cumulative daily energy consumption
data['daily_cumulative_energy'] = data.groupby('day')['Appliances'].cumsum()

# Average energy consumption for the same hour
data['hourly_avg_energy'] = data.groupby('hour')['Appliances'].transform('mean')

# Adjusted temperature using wind chill
data['adjusted_temp'] = data['T_out'] - (data['Windspeed'] * 0.7)

# Humidity index
data['humidity_index'] = data['T_out'] * 0.1 + 0.9 * data['RH_out']

# Statistical features
data['temp_mean'] = data[['T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9']].mean(axis=1)
data['temp_std'] = data[['T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9']].std(axis=1)
data['temp_range'] = data[['T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9']].max(axis=1) - \
                     data[['T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9']].min(axis=1)

data['humidity_mean'] = data[['RH_1', 'RH_2', 'RH_3', 'RH_4', 'RH_5', 'RH_6', 'RH_7', 'RH_8', 'RH_9']].mean(axis=1)

"""# **Exploratory Data Analyis**"""

import seaborn as sns
import matplotlib.pyplot as plt

# Distribution plot
sns.histplot(data['Appliances'], kde=True, bins=50)
plt.title("Distribution of Energy Consumption (Appliances)")
plt.xlabel("Energy Consumption (Wh)")
plt.ylabel("Frequency")
plt.show()

# Correlation heatmap
correlation_matrix = data.corr()
plt.figure(figsize=(15, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()

# Target correlation
target_corr = correlation_matrix['Appliances'].sort_values(ascending=False)
print("Correlation with Appliances:\n", target_corr)

# Average energy consumption by hour
hourly_data = data.groupby('hour')['Appliances'].mean().reset_index()
plt.figure(figsize=(10, 5))
sns.lineplot(data=hourly_data, x='hour', y='Appliances')
plt.title("Average Energy Consumption by Hour")
plt.xlabel("Hour of the Day")
plt.ylabel("Energy Consumption (Wh)")
plt.show()

# By day
daily_data = data.groupby('day')['Appliances'].mean().reset_index()
plt.figure(figsize=(10, 5))
sns.barplot(data=daily_data, x='day', y='Appliances', palette='viridis')
plt.title("Average Energy Consumption by Day of the Month")
plt.xlabel("Day")
plt.ylabel("Energy Consumption (Wh)")
plt.show()

# By weekday
weekday_data = data.groupby('weekday')['Appliances'].mean().reset_index()
plt.figure(figsize=(10, 5))
sns.barplot(data=weekday_data, x='weekday', y='Appliances', palette='viridis')
plt.title("Average Energy Consumption by Day of the Week")
plt.xlabel("Weekday (0=Monday, 6=Sunday)")
plt.ylabel("Energy Consumption (Wh)")
plt.show()

sns.scatterplot(data=data, x='T_out', y='T1', hue='Appliances', palette='coolwarm')
plt.title("Indoor Temperature (T1) vs Outdoor Temperature (To)")
plt.xlabel("Outdoor Temperature (To)")
plt.ylabel("Indoor Temperature (T1)")
plt.show()

# Box plots for temperatures
features_temp = ['T1', 'T2', 'T3', 'T4', 'T5', 'T_out']
plt.figure(figsize=(15, 7))
sns.boxplot(data=data[features_temp], palette="Set2")
plt.title("Box Plot for Temperature Features")
plt.show()

# Box plots for humidity
features_humidity = ['RH_1', 'RH_2', 'RH_3', 'RH_4', 'RH_5', 'RH_out']
plt.figure(figsize=(15, 7))
sns.boxplot(data=data[features_humidity], palette="Set2")
plt.title("Box Plot for Humidity Features")
plt.show()

from scipy.stats import zscore

# Calculate Z-scores
z_scores = data.select_dtypes(include=['float64', 'int64']).apply(zscore)

# Identify rows with Z-scores beyond threshold
threshold = 3
outliers_z = (z_scores.abs() > threshold)

# Count outliers per column
outlier_counts_z = outliers_z.sum()
print("Outliers per column (Z-score):\n", outlier_counts_z)

# Calculate Q1, Q3, and IQR
# Convert boolean columns to numeric before calculating quantiles
for col in data.select_dtypes(include=['bool']).columns:
    data[col] = data[col].astype(int)

Q1 = data.quantile(0.25)
Q3 = data.quantile(0.75)
IQR = Q3 - Q1

# Identify outliers
outliers_iqr = ((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR)))

# Count outliers per column
outlier_counts_iqr = outliers_iqr.sum()
print("Outliers per column (IQR):\n", outlier_counts_iqr)

import matplotlib.pyplot as plt

# Create box plots for numerical columns
data.select_dtypes(include=['float64', 'int64']).boxplot(figsize=(12, 6))
plt.title("Box Plot of Numerical Features")
plt.xticks(rotation=45)
plt.show()

# Calculate proportion of outliers
outlier_proportion = outliers_iqr.any(axis=1).mean()
print(f"Proportion of outliers in the dataset: {outlier_proportion:.2%}")

# Remove outliers (IQR method)
data_no_outliers = data[~outliers_iqr.any(axis=1)]

# Display dataset shape before and after
print("Original data shape:", data.shape)
print("Data shape after removing outliers:", data_no_outliers.shape)

# Correlation matrix of the dataset with outliers
correlation_with_outliers = data.corr()

# Correlation matrix of the dataset without outliers
correlation_without_outliers = data_no_outliers.corr()

# Display differences
print("Correlation matrix differences:\n", correlation_with_outliers - correlation_without_outliers)

import seaborn as sns

# Example: Distribution of Appliances with and without outliers
sns.kdeplot(data['Appliances'], label='With Outliers', shade=True)
sns.kdeplot(data_no_outliers['Appliances'], label='Without Outliers', shade=True)
plt.legend()
plt.title("Distribution of Appliances (With vs Without Outliers)")
plt.show()

"""# **Model building**"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer

# Define features and target
# Exclude 'date' column from features (since it's already the index)
X = data.drop(columns=['Appliances', 'date'])  # Features  # Exclude 'date' here
y = data['Appliances']                # Target

# ... (rest of your code) ...
# Standard Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Verify shapes
print("Training set shape:", X_train.shape, y_train.shape)
print("Testing set shape:", X_test.shape, y_test.shape)

# Libraries for machine learning models
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.svm import SVR

# Deep learning for LSTM
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

# Evaluation metrics
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

def evaluate_model(y_test, y_pred):
    """
    Evaluate the model and print metrics.
    """
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    print(f"MAE: {mae:.2f}, RMSE: {rmse:.2f}, RÂ²: {r2:.2f}")
    return mae, rmse, r2

# Linear Regression
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
lr_pred = lr_model.predict(X_test)

print("Linear Regression Results:")
evaluate_model(y_test, lr_pred)

# Decision Tree
dt_model = DecisionTreeRegressor(max_depth=10, random_state=42)
dt_model.fit(X_train, y_train)
dt_pred = dt_model.predict(X_test)

print("Decision Tree Results:")
evaluate_model(y_test, dt_pred)

# Random Forest
rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)

print("Random Forest Results:")
evaluate_model(y_test, rf_pred)

# XGBoost
xgb_model = XGBRegressor(n_estimators=100, max_depth=10, learning_rate=0.1, random_state=42)
xgb_model.fit(X_train, y_train)
xgb_pred = xgb_model.predict(X_test)

print("XGBoost Results:")
evaluate_model(y_test, xgb_pred)

# SVR
svr_model = SVR(kernel='rbf', C=10, epsilon=0.1)
svr_model.fit(X_train, y_train)
svr_pred = svr_model.predict(X_test)

print("SVR Results:")
evaluate_model(y_test, svr_pred)

# Reshape data for LSTM (time-series-like structure)
X_train_lstm = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test_lstm = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

# LSTM Model
lstm_model = Sequential([
    LSTM(50, activation='relu', input_shape=(X_train_lstm.shape[1], 1)),
    Dropout(0.2),
    Dense(1)
])

# Compile LSTM
lstm_model.compile(optimizer='adam', loss='mse')

# Train LSTM
lstm_model.fit(X_train_lstm, y_train, epochs=50, batch_size=32, verbose=2)

# Predict and evaluate LSTM
lstm_pred = lstm_model.predict(X_test_lstm)
print("LSTM Results:")
evaluate_model(y_test, lstm_pred)

import pandas as pd
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor

# Convert 'date' to datetime and set it as the index
data['date'] = pd.to_datetime(data['date'])
data.set_index('date', inplace=True)

# Use the 'Appliances' column for time-series modeling
y = data['Appliances']

# Split data into train and test sets
train_size = int(len(y) * 0.8)
train, test = y[:train_size], y[train_size:]

# Fit ARIMA model
arima_model = ARIMA(train, order=(2, 1, 2))  # (p, d, q) values can be tuned
arima_fitted = arima_model.fit()

# Predict
arima_pred = arima_fitted.forecast(steps=len(test))

# Evaluate
print("ARIMA Results:")
evaluate_model(test, arima_pred)
 # Fit SARIMA model
sarima_model = SARIMAX(train, order=(2, 1, 2), seasonal_order=(1, 1, 1, 24))  # Seasonal order: (P, D, Q, S)
sarima_fitted = sarima_model.fit(disp=False)

# Predict
sarima_pred = sarima_fitted.forecast(steps=len(test))

# Evaluate
print("SARIMA Results:")
evaluate_model(test, sarima_pred)

from sklearn.model_selection import train_test_split

# Features and target
X = data.drop(columns=['Appliances'])  # Features
y = data['Appliances']                # Target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize AdaBoost Regressor
adaboost_model = AdaBoostRegressor(
    estimator=DecisionTreeRegressor(max_depth=5),  # Changed 'base_estimator' to 'estimator'
    n_estimators=50,
    random_state=42
)

# Fit the model
adaboost_model.fit(X_train, y_train)

# Predict
adaboost_pred = adaboost_model.predict(X_test)

# Evaluate
print("AdaBoost Results:")
evaluate_model(y_test, adaboost_pred)

# Storing results in a dictionary
results = {
    "Linear Regression": evaluate_model(y_test, lr_pred),
    "Decision Tree": evaluate_model(y_test, dt_pred),
    "Random Forest": evaluate_model(y_test, rf_pred),
    "XGBoost": evaluate_model(y_test, xgb_pred),
    "SVR": evaluate_model(y_test, svr_pred),
    "LSTM": evaluate_model(y_test, lstm_pred)
}
# Convert results into a DataFrame for comparison
results_df = pd.DataFrame(results, index=["MAE", "RMSE", "RÂ²"]).T
print(results_df)
results = {
    "ARIMA": evaluate_model(test, arima_pred),
    "SARIMA": evaluate_model(test, sarima_pred),
    "AdaBoost": evaluate_model(y_test, adaboost_pred)
}

# Convert to DataFrame
results_df = pd.DataFrame(results, index=["MAE", "RMSE", "RÂ²"]).T
print(results_df)

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np
# Define features and target
X = data.drop(columns=['Appliances'])  # Features
y = data['Appliances']                # Target

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
# Initialize KNN Regressor
knn_model = KNeighborsRegressor(n_neighbors=5, weights='uniform', metric='minkowski', p=2)  # Euclidean distance

# Fit the model
knn_model.fit(X_train_scaled, y_train)

# Predict on test data
knn_pred = knn_model.predict(X_test_scaled)
# Evaluation metrics
def evaluate_model(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    print(f"MAE: {mae:.2f}, RMSE: {rmse:.2f}, RÂ²: {r2:.2f}")
    return mae, rmse, r2

# Evaluate KNN
print("KNN Results:")
evaluate_model(y_test, knn_pred)

"""# **Hyperparameter tuning**"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV
import joblib

# Define the Random Forest model
rf = RandomForestRegressor(random_state=42)

# Define a smaller parameter grid for quick tuning
param_grid = {
    'n_estimators': [50, 100],  # Smaller number of trees for quicker tuning
    'max_depth': [10, 20, None],  # Depth of the tree
    'min_samples_split': [2, 5],  # Minimum samples required to split
    'min_samples_leaf': [1, 2],  # Minimum samples in a leaf
    'max_features': ['sqrt', 'log2']  # Number of features to consider for the best split
}

# Use fewer cross-validation folds for quicker results
grid_search = GridSearchCV(
    estimator=rf,
    param_grid=param_grid,
    cv=3,  # 3-fold cross-validation
    scoring='neg_mean_absolute_error',  # Metric for evaluation
    verbose=1,
    n_jobs=-1  # Use all available cores
)

# Fit the grid search to the training data
grid_search.fit(X_train, y_train)

# Save the best model
best_rf_model = grid_search.best_estimator_
joblib.dump(best_rf_model, 'best_random_forest_model.pkl')

# Display best parameters and score
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", -grid_search.best_score_)

from sklearn.ensemble import RandomForestRegressor

# Retrain the model with the best parameters
best_rf_model = RandomForestRegressor(
    max_depth=20,
    max_features='sqrt',
    min_samples_leaf=1,
    min_samples_split=2,
    n_estimators=100,
    random_state=42
)

# Fit the model on the training data
best_rf_model.fit(X_train, y_train)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Predict on the test set
y_pred = best_rf_model.predict(X_test)

# Calculate metrics
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"Test Set Performance:")
print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"RÂ² Score: {r2:.2f}")

from sklearn.base import BaseEstimator, TransformerMixin
import pandas as pd
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.svm import SVR
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.model_selection import GridSearchCV
import joblib
from sklearn.pipeline import Pipeline

# Create a custom transformer class
class FeatureEngineeringTransformer(BaseEstimator, TransformerMixin):
    def __init__(self):
        pass  # Add any initialization parameters you need

    def fit(self, X, y=None):
        # Fit the transformer. For feature engineering, this might be a no-op.
        return self

    def transform(self, X):
        # Perform your feature engineering logic here
        # ...
        return X  # Return the modified DataFrame

    def fit_transform(self, X, y=None):
        # Combine fit and transform for efficiency
        self.fit(X, y)
        return self.transform(X)

# Create an instance of your transformer
feature_engineering_step = FeatureEngineeringTransformer()

# Combine preprocessing steps into a pipeline
preprocessing_pipeline = Pipeline([
    ('scaler', scaler),
    ('feature_engineering', feature_engineering_step),
])

# Apply preprocessing to training data
X_train_transformed = preprocessing_pipeline.fit_transform(X_train)

# Save the preprocessing pipeline
joblib.dump(preprocessing_pipeline, 'preprocessing_pipeline.pkl')

# Train the model on the transformed data
best_rf_model.fit(X_train_transformed, y_train)

# Save the trained model
joblib.dump(best_rf_model, 'random_forest_model.pkl')

import pandas as pd
from sklearn.preprocessing import StandardScaler
import joblib

# Define the prediction function
def predict_energy(user_input, X_train):
    """
    Predicts energy consumption using the saved Random Forest model.

    Parameters:
        user_input (dict): User input containing feature values.
        X_train (pd.DataFrame): Training data used for scaling.

    Returns:
        float: Predicted energy consumption in Wh.
    """
    # Load the trained Random Forest model
    model = joblib.load('random_forest_model.pkl')

    # Reinitialize the scaler and fit it on the training data
    scaler = StandardScaler()
    scaler.fit(X_train)  # Use training data to fit the scaler

    # Convert user input into a DataFrame
    user_input_df = pd.DataFrame([user_input])

    # Scale the input data using the same scaler
    scaled_input = scaler.transform(user_input_df)

    # Predict using the Random Forest model
    prediction = model.predict(scaled_input)

    return prediction[0]

user_input = {
    "lights": 10,
    "T1": 19.0,
    "RH_1": 47.0,
    "T2": 21.0,
    "RH_2": 44.0,
    "T3": 20.0,
    "RH_3": 50.0,
    "T4": 22.0,
    "RH_4": 45.0,
    "T5": 23.0,
    "RH_5": 40.0,
    "T6": 18.0,
    "RH_6": 55.0,
    "T7": 19.0,
    "RH_7": 48.0,
    "T8": 20.5,
    "RH_8": 42.0,
    "T9": 21.5,
    "RH_9": 46.0,
    "To": 15.0,
    "Pressure": 1013.0,
    "RH_out": 85.0,
    "Wind speed": 2.0,
    "Visibility": 40.0,
    "Tdewpoint": 10.0,
    "rv1": 0.5,
    "rv2": 0.5
}

# Predict energy consumption
predicted_energy = predict_energy(user_input, X_train)

# Display the result
print(f"Predicted energy consumption: {predicted_energy:.2f} Wh")

import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler

# Train the Random Forest model
rf_model = RandomForestRegressor(
    max_depth=20,
    max_features='sqrt',
    min_samples_leaf=1,
    min_samples_split=2,
    n_estimators=100,
    random_state=42
)
rf_model.fit(X_train, y_train)

# Define the prediction function
def predict_energy_live(user_input, rf_model, X_train):
    """
    Predicts energy consumption using the trained Random Forest model.

    Parameters:
        user_input (dict): User input containing feature values.
        rf_model (object): Trained Random Forest model.
        X_train (pd.DataFrame): Training data used for scaling.

    Returns:
        float: Predicted energy consumption in Wh.
    """
    # Reinitialize the scaler and fit it on the training data
    scaler = StandardScaler()
    scaler.fit(X_train)  # Use training data to fit the scaler

    # Convert user input into a DataFrame with the correct column names
    user_input_df = pd.DataFrame([user_input], columns=X_train.columns)

    # Scale the input data using the same scaler
    scaled_input = scaler.transform(user_input_df)

    # Predict using the Random Forest model
    prediction = rf_model.predict(scaled_input)

    return prediction[0]

# Example user input
user_input = {
    "lights": 10,
    "T1": 19.0,
    "RH_1": 47.0,
    "T2": 21.0,
    "RH_2": 44.0,
    "T3": 20.0,
    "RH_3": 50.0,
    "T4": 22.0,
    "RH_4": 45.0,
    "T5": 23.0,
    "RH_5": 40.0,
    "T6": 18.0,
    "RH_6": 55.0,
    "T7": 19.0,
    "RH_7": 48.0,
    "T8": 20.5,
    "RH_8": 42.0,
    "T9": 21.5,
    "RH_9": 46.0,
    "To": 15.0,
    "Pressure": 1013.0,
    "RH_out": 85.0,
    "Wind speed": 2.0,
    "Visibility": 40.0,
    "Tdewpoint": 10.0,
    "rv1": 0.5,
    "rv2": 0.5
}

# Predict energy consumption
predicted_energy = predict_energy_live(user_input, rf_model, X_train)

# Display the result
print(f"Predicted energy consumption: {predicted_energy:.2f} Wh")

import pandas as pd

# Predict energy consumption for the test set
y_pred = rf_model.predict(X_test)

# Combine actual and predicted values in a DataFrame
results_df = pd.DataFrame({
    'Actual': y_test,
    'Predicted': y_pred,
    'Residuals': y_test - y_pred
})

# Summary statistics
print("Summary Statistics:")
print(results_df.describe())

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8, 6))
sns.scatterplot(x='Actual', y='Predicted', data=results_df, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Line of perfect predictions
plt.title('Actual vs Predicted Energy Consumption')
plt.xlabel('Actual Energy Consumption (Wh)')
plt.ylabel('Predicted Energy Consumption (Wh)')
plt.show()

plt.figure(figsize=(8, 6))
sns.histplot(results_df['Residuals'], kde=True, bins=30, color='blue', alpha=0.7)
plt.title('Distribution of Residuals')
plt.xlabel('Residuals (Actual - Predicted)')
plt.ylabel('Frequency')
plt.axvline(0, color='red', linestyle='--', label='Zero Error')
plt.legend()
plt.show()

plt.figure(figsize=(8, 6))
sns.scatterplot(x=results_df['Predicted'], y=results_df['Residuals'], alpha=0.6)
plt.axhline(0, color='red', linestyle='--')
plt.title('Residuals vs Predicted Values')
plt.xlabel('Predicted Energy Consumption (Wh)')
plt.ylabel('Residuals')
plt.show()

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"R-squared (RÂ²): {r2:.2f}")

import joblib

# Train the Random Forest model
rf_model = RandomForestRegressor(
    max_depth=20,
    max_features='sqrt',
    min_samples_leaf=1,
    min_samples_split=2,
    n_estimators=100,
    random_state=42
)
rf_model.fit(X_train, y_train)

# Save the model to a file
joblib.dump(rf_model, 'random_forest_model.pkl')

print("Model saved as random_forest_model.pkl")

from google.colab import files

# Download the saved model
files.download('/content/random_forest_model.pkl')

